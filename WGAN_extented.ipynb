{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1UoVlHR3_3uqt85huq44NOXjNyViuxwTA","authorship_tag":"ABX9TyP+pOZWgreb4FK14+XyM449"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3ChveA0ybpM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.autograd import grad as torch_grad\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.decomposition import PCA\n","\n","import math"]},{"cell_type":"code","source":["df=pd.read_csv(\"/content/drive/MyDrive/FYP/Stock Market Group/DataSet/AAPL/AAPL_data_with_indicators.csv\")\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"0Tg174cdy9az","executionInfo":{"status":"ok","timestamp":1713373051127,"user_tz":-330,"elapsed":2674,"user":{"displayName":"Madara Gunarathna","userId":"05432227937498129950"}},"outputId":"1e3c9029-8978-4b69-8b73-2ca187d8a8cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              Datetime        Open        High         Low       Close  \\\n","0  2023-04-03 13:30:00  164.270004  165.559998  164.220001  165.496704   \n","1  2023-04-03 13:35:00  165.494995  165.849899  165.360001  165.725006   \n","2  2023-04-03 13:40:00  165.732300  165.732300  165.440002  165.449997   \n","3  2023-04-03 13:45:00  165.440002  165.869995  165.229996  165.869995   \n","4  2023-04-03 13:50:00  165.869995  165.889999  165.270004  165.300003   \n","\n","    Adj Close   Volume  SMA  wma  ema  ...  strength_line  signal_line  fi  \\\n","0  165.496704  3833951  NaN  NaN  NaN  ...            NaN          NaN NaN   \n","1  165.725006  1398850  NaN  NaN  NaN  ...            NaN          NaN NaN   \n","2  165.449997  1077282  NaN  NaN  NaN  ...            NaN          NaN NaN   \n","3  165.869995  1138968  NaN  NaN  NaN  ...            NaN          NaN NaN   \n","4  165.300003  1160507  NaN  NaN  NaN  ...            NaN          NaN NaN   \n","\n","   kline  dline       atr  aroon_up  aroon_down  roc        psar  \n","0    NaN    NaN  1.339996       NaN         NaN  NaN  164.220001  \n","1    NaN    NaN  1.279275       NaN         NaN  NaN  164.220001  \n","2    NaN    NaN  1.208777       NaN         NaN  NaN  164.220001  \n","3    NaN    NaN  1.168150       NaN         NaN  NaN  164.285197  \n","4    NaN    NaN  1.128996       NaN         NaN  NaN  164.380285  \n","\n","[5 rows x 27 columns]"],"text/html":["\n","  <div id=\"df-73ee44f4-2a8a-45cf-9629-27972a3d7dfd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Datetime</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Adj Close</th>\n","      <th>Volume</th>\n","      <th>SMA</th>\n","      <th>wma</th>\n","      <th>ema</th>\n","      <th>...</th>\n","      <th>strength_line</th>\n","      <th>signal_line</th>\n","      <th>fi</th>\n","      <th>kline</th>\n","      <th>dline</th>\n","      <th>atr</th>\n","      <th>aroon_up</th>\n","      <th>aroon_down</th>\n","      <th>roc</th>\n","      <th>psar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023-04-03 13:30:00</td>\n","      <td>164.270004</td>\n","      <td>165.559998</td>\n","      <td>164.220001</td>\n","      <td>165.496704</td>\n","      <td>165.496704</td>\n","      <td>3833951</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.339996</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>164.220001</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-04-03 13:35:00</td>\n","      <td>165.494995</td>\n","      <td>165.849899</td>\n","      <td>165.360001</td>\n","      <td>165.725006</td>\n","      <td>165.725006</td>\n","      <td>1398850</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.279275</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>164.220001</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-04-03 13:40:00</td>\n","      <td>165.732300</td>\n","      <td>165.732300</td>\n","      <td>165.440002</td>\n","      <td>165.449997</td>\n","      <td>165.449997</td>\n","      <td>1077282</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.208777</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>164.220001</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-04-03 13:45:00</td>\n","      <td>165.440002</td>\n","      <td>165.869995</td>\n","      <td>165.229996</td>\n","      <td>165.869995</td>\n","      <td>165.869995</td>\n","      <td>1138968</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.168150</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>164.285197</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-04-03 13:50:00</td>\n","      <td>165.869995</td>\n","      <td>165.889999</td>\n","      <td>165.270004</td>\n","      <td>165.300003</td>\n","      <td>165.300003</td>\n","      <td>1160507</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.128996</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>164.380285</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 27 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73ee44f4-2a8a-45cf-9629-27972a3d7dfd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-73ee44f4-2a8a-45cf-9629-27972a3d7dfd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-73ee44f4-2a8a-45cf-9629-27972a3d7dfd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6af42bab-0b07-4207-816e-f61d607e20f0\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6af42bab-0b07-4207-816e-f61d607e20f0')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6af42bab-0b07-4207-816e-f61d607e20f0 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Data Pre-process"],"metadata":{"id":"kPU8Fw-k2dc1"}},{"cell_type":"code","source":["def preprocess(df):\n","  df.index = pd.to_datetime(df['Datetime'])\n","  cols = list(df)[1:]\n","  df_for_training = df[cols].astype(float)\n","  df_for_training =df_for_training.dropna()\n","  labels_df = df_for_training[\"Close\"]\n","  features_df = df_for_training.drop(columns=[\"Close\"])\n","\n","  return features_df,labels_df"],"metadata":{"id":"28dX2bgm1qXb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sliding Window"],"metadata":{"id":"3M88-aJ82Zn7"}},{"cell_type":"code","source":["def sliding_window(x, y, window):\n","  x_ = []\n","  y_ = []\n","  y_gan = []\n","  for i in range(window, x.shape[0]-4):\n","    tmp_x = x[i - window: i, :]\n","    tmp_y = y[i:i+5].flatten()\n","    tmp_y_gan = y[i - window: i + 5]\n","    x_.append(tmp_x)\n","    y_.append(tmp_y)\n","    y_gan.append(tmp_y_gan)\n","  x_ = np.array(x_)\n","  y_ = np.array(y_)\n","  y_gan = np.array(y_gan)\n","  return x_, y_, y_gan"],"metadata":{"id":"cuPff8Ko2YwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TODO: save scalers\n","def normalize(train_x,train_y):\n","  x_scaler = MinMaxScaler(feature_range = (0, 1))\n","  y_scaler = MinMaxScaler(feature_range = (0, 1))\n","  train_x = x_scaler.fit_transform(train_x)\n","\n","  train_y = (train_y.values.reshape(-1, 1))\n","\n","  return train_x,train_y\n"],"metadata":{"id":"F3zRJRSz3AXM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Split datasets"],"metadata":{"id":"nhxl23ko4JLM"}},{"cell_type":"code","source":["def split_dataset(features_df,labels_df,y_gan):\n","    train_x = features_df[:training_duration]\n","    train_y = labels_df[:training_duration]#.values.reshape(-1,1)\n","    train_y_gan = y_gan[:training_duration]#.values.reshape(-1,1)\n","\n","    test_x = features_df[training_duration:training_duration+testing_duration]\n","    test_y = labels_df[training_duration:training_duration+testing_duration]#.values.reshape(-1,1)\n","    test_y_gan = y_gan[training_duration:training_duration+testing_duration]#.values.reshape(-1,1)\n","\n","    print(f'trainX: {train_x.shape} trainY: {train_y.shape}')\n","    print(f'testX: {test_x.shape} testY: {test_y.shape}')\n","\n","    train_x = torch.from_numpy(np.array(train_x)).float()\n","    train_y = torch.from_numpy(np.array(train_y)).float()\n","    train_y_gan = torch.from_numpy(np.array(train_y_gan)).float()\n","    test_x = torch.from_numpy(np.array(test_x)).float()\n","    test_y = torch.from_numpy(np.array(test_y)).float()\n","    test_y_gan = torch.from_numpy(np.array(test_y_gan)).float()\n","\n","\n","    return train_x,train_y,train_y_gan,test_x,test_y,test_y_gan"],"metadata":{"id":"4aCqHSSD4L4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Implementation"],"metadata":{"id":"Gjc8DbEr4X--"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","  def __init__(self, input_size):\n","    super().__init__()\n","\n","    # 3 GRU layers, input_size = features\n","    self.gru_1 = nn.GRU(input_size, 1024, batch_first=True)\n","    self.gru_2 = nn.GRU(1024, 512, batch_first = True)\n","    self.gru_3 = nn.GRU(512, 256, batch_first = True)\n","    self.gru_4 = nn.GRU(256, 128, batch_first = True)\n","    # 3 Dense Layers\n","    self.linear_1 = nn.Linear(256, 128)\n","    self.linear_2 = nn.Linear(128, 64)\n","    self.linear_3 = nn.Linear(64, 5)\n","\n","    self.dropout = nn.Dropout(0.2)\n","\n","\n","  def forward(self, x):\n","    use_cuda = 1\n","    device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n","    h0 = torch.zeros(1, x.size(0), 1024).to(device) # initial hidden state for the 1st GRU Layer - (num of layers in the GRU, batch size, num of hidden units in the GRU)\n","    out_gru_1, _ = self.gru_1(x, h0)\n","    out_gru_1 = self.dropout(out_gru_1)\n","\n","    h1 = torch.zeros(1, x.size(0), 512).to(device)\n","    out_gru_2, _ = self.gru_2(out_gru_1, h1)\n","    out_gru_2 = self.dropout(out_gru_2)\n","\n","    h2 = torch.zeros(1, x.size(0), 256).to(device)\n","    out_gru_3, _ = self.gru_3(out_gru_2, h2)\n","    out_gru_3 = self.dropout(out_gru_3)\n","\n","    h3 = torch.zeros(1, x.size(0), 128).to(device)\n","    out_gru_4, _ = self.gru_4(out_gru_3, h3)\n","    out_gru_4 = self.dropout(out_gru_4)\n","\n","\n","    out_dense_1 = self.linear_1(out_gru_3[:, -1, :])\n","    out_dense_2 = self.linear_2(out_dense_1)\n","    out_dense_3 = self.linear_3(out_dense_2)\n","\n","    return out_dense_3,out_gru_4"],"metadata":{"id":"X0KKHf5t4bRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # 3 1D Conv layers\n","    self.conv1 = nn.Conv1d(sliding_window_size+5, 32, kernel_size = 5, stride = 1, padding = 'same')\n","    self.conv2 = nn.Conv1d(32, 64, kernel_size = 5, stride = 1, padding = 'same')\n","    self.conv3 = nn.Conv1d(64, 128, kernel_size = 5, stride = 1, padding = 'same')\n","\n","    # 3 linear layers\n","    self.linear1 = nn.Linear(128, 220)\n","    self.linear2 = nn.Linear(220, 220)\n","    self.linear3 = nn.Linear(220, 5)\n","\n","    self.leaky = nn.LeakyReLU(0.01)\n","    self.relu = nn.ReLU()\n","    self.tanh = nn.Tanh()\n","\n","  def forward(self, x):\n","    conv1 = self.conv1(x)\n","    conv1 = self.leaky(conv1)\n","    conv2 = self.conv2(conv1)\n","    conv2 = self.leaky(conv2)\n","    conv3 = self.conv3(conv2)\n","    conv3 = self.leaky(conv3)\n","\n","    flatten_x =  conv3.reshape(conv3.shape[0], conv3.shape[1])\n","\n","    out_1 = self.linear1(flatten_x)\n","    out_1 = self.leaky(out_1)\n","    out_2 = self.linear2(out_1)\n","    out_2 = self.relu(out_2)\n","    out_3 = self.linear3(out_2)\n","\n","    return out_3"],"metadata":{"id":"QqHjnDw14e9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grad_penalty_fnc(real_data, gen_data,D,cuda,gp_weight):\n","    batch_size = real_data.size()[0]\n","    t = torch.rand((batch_size, 1, 1), requires_grad=True)\n","    t = t.expand_as(real_data)\n","\n","    if cuda:\n","        t = t.cuda()\n","\n","    # mixed sample from real and fake; make approx of the 'true' gradient norm\n","    interpol = t * real_data.data + (1-t) * gen_data.data\n","\n","    if cuda:\n","        interpol = interpol.cuda()\n","    prob_interpol = D(interpol)\n","    torch.autograd.set_detect_anomaly(True)\n","    gradients = torch_grad(outputs=prob_interpol, inputs=interpol,\n","                           grad_outputs=torch.ones(prob_interpol.size()).cuda() if cuda else torch.ones(\n","                               prob_interpol.size()), create_graph=True, retain_graph=True)[0]\n","    gradients = gradients.view(batch_size, -1)\n","    #grad_norm = torch.norm(gradients, dim=1).mean()\n","    #self.losses['gradient_norm'].append(grad_norm.item())\n","\n","    # add epsilon for stability\n","    eps = 1e-10\n","    gradients_norm = torch.sqrt(torch.sum(gradients**2, dim=1, dtype=torch.double) + eps)\n","    #gradients = gradients.cpu()\n","    # comment: precision is lower than grad_norm (think that is double) and gradients_norm is float\n","    final = gp_weight * (torch.max(torch.zeros(1,dtype=torch.double).cuda() if cuda else torch.zeros(1,dtype=torch.double), gradients_norm.mean() - 1) ** 2), gradients_norm.mean().item()\n","    return final\n"],"metadata":{"id":"7hjH7T3m4iBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train Model"],"metadata":{"id":"R-WXQFqf4v5b"}},{"cell_type":"code","source":["def train(train_x_slide,train_y_gan,scaled_x,sliding_window_size,device,cuda):\n","  batch_size = 128\n","  learning_rate = 0.000115\n","  num_epochs = 100\n","  critic_iterations = 5\n","  eraly_exit = 10\n","  treshold = 0.1\n","  count = 0\n","\n","  trainDataloader = DataLoader(TensorDataset(train_x_slide, train_y_gan), batch_size = batch_size, shuffle = False)\n","\n","  # Give number of features to the G\n","  modelG = Generator(scaled_x.shape[1]).to(device)\n","  modelD = Discriminator().to(device)\n","\n","  #weight_decay-L2 penalty to the weights\n","  optimizerG = torch.optim.Adam(modelG.parameters(), lr = learning_rate, betas = (0.0, 0.9), weight_decay = 1e-3)\n","  optimizerD = torch.optim.Adam(modelD.parameters(), lr = learning_rate, betas = (0.0, 0.9), weight_decay = 1e-3)\n","\n","  # optimizerG = torch.optim.RMSprop(modelG.parameters(), lr = learning_rate, weight_decay = 1e-3)\n","  # optimizerD = torch.optim.RMSprop(modelD.parameters(), lr = learning_rate, weight_decay = 1e-3)\n","\n","  histG = np.zeros(num_epochs)\n","  histD = np.zeros(num_epochs)\n","  count = 0\n","\n","  #k=0\n","  i=0\n","  for epoch in range(num_epochs):\n","    loss_G = []\n","    loss_D = []\n","    for (x, y) in trainDataloader:\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      fake_data,_ = modelG(x)\n","#         print(y.shape,fake_data.shape)\n","#         print(y[:, :sliding_window_size, :].shape, fake_data.reshape(-1, 1, 1).shape)\n","      i=1\n","      fake_data = torch.cat([y[:, :sliding_window_size, :], fake_data.reshape(-1, 5, 1)], axis = 1)\n","\n","      for _ in range(critic_iterations):\n","        critic_real = modelD(y)\n","        critic_fake = modelD(fake_data)\n","        grad_penalty, grad_norm_ = grad_penalty_fnc(y, fake_data,modelD,cuda,10)\n","        # Take probability mean of whole batch.\n","        lossD = -(torch.mean(critic_real) - torch.mean(critic_fake)) + grad_penalty\n","\n","        modelD.zero_grad()\n","        lossD.backward(retain_graph = True)\n","        optimizerD.step()\n","\n","      output_fake = modelD(fake_data)\n","      lossG = -torch.mean(output_fake)\n","\n","      modelG.zero_grad() # zeroing the gradients\n","      lossG.backward() # computing the gradients\n","      optimizerG.step() # updating the parameters\n","\n","      loss_D.append(lossD.item())\n","      loss_G.append(lossG.item())\n","\n","    histG[epoch] = sum(loss_G)\n","    histD[epoch] = sum(loss_D)\n","\n","    # Check if the loss exceeds the threshold\n","#     if sum(loss_D) > treshold:\n","#         count+=1\n","#         if count>= eraly_exit:\n","#             print(f'Early exit at epoch {epoch+1} due to loss exceeding the threshold.')\n","#             break\n","    print(f'[{epoch+1}/{num_epochs}] LossD: {sum(loss_D)} LossG:{sum(loss_G)}')\n","  return modelG"],"metadata":{"id":"euOIjEZQ4y9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate model"],"metadata":{"id":"KLtjQSY36S8M"}},{"cell_type":"code","source":["def evaluateModel(modelG,train_x_slide,test_x_slide,train_y_slide,test_y_slide):\n","  modelG.eval()\n","  pred_y_train,_ = modelG(train_x_slide.to(device))\n","  pred_y_test,_ = modelG(test_x_slide.to(device))\n","\n","\n","  y_train_true =train_y_slide\n","  y_train_pred = pred_y_train.cpu().detach().numpy()\n","\n","  y_test_true = test_y_slide\n","  y_test_pred = pred_y_test.cpu().detach().numpy()\n","\n","  plt.figure(figsize=(12, 8))\n","  plt.plot(y_train_true, color = 'black', label = 'Acutal Price')\n","  plt.plot(y_train_pred, color = 'blue', label = 'Predict Price')\n","  plt.title('WGAN-GP prediction training dataset')\n","  plt.ylabel('BTC')\n","  plt.xlabel('5 min time periods')\n","  plt.legend(loc = 'upper right')\n","\n","  MSE = mean_squared_error(y_train_true, y_train_pred)\n","  RMSE = math.sqrt(MSE)\n","  print(f'Training dataset RMSE:{RMSE}')\n","\n","\n","  plt.figure(figsize=(12, 8))\n","  plt.plot(y_test_true, color = 'black', label = 'Acutal Price')\n","  plt.plot(y_test_pred, color = 'blue', label = 'Predict Price')\n","  plt.title('WGAN-GP prediction testing dataset')\n","  plt.ylabel('BTC')\n","  plt.xlabel('5 min time periods')\n","  plt.legend(loc = 'upper right')\n","\n","  MSE = mean_squared_error(y_test_true, y_test_pred)\n","  RMSE = math.sqrt(MSE)\n","  print(f'Testing dataset RMSE:{RMSE}')"],"metadata":{"id":"Vq8TplTr6XNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#XGB input pre-process"],"metadata":{"id":"stQx5e4MfgS0"}},{"cell_type":"code","source":["def xgb_input(train_input_features):\n","  # Create column names for features\n","  feature_names = [f\"X_feature_{i}\" for i in range(train_input_features.shape[1])]\n","\n","  # Combine column names\n","  column_names = feature_names\n","\n","  # Move the tensor to the CPU before converting to NumPy\n","  train_input_features_cpu = train_input_features.cpu().detach().numpy()\n","\n","  # Create a DataFrame with the concatenated array and column names\n","  features_df = pd.DataFrame(train_input_features_cpu, columns=column_names)\n","\n","  # Specify the path where you want to save the CSV file\n","  csv_path = \"gan_output.csv\"\n","\n","  # Save the DataFrame to a CSV file\n","  features_df.to_csv(csv_path, index=False)"],"metadata":{"id":"nelrnihrfnpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Elliot input pre-process"],"metadata":{"id":"dFcP0LI6gVnw"}},{"cell_type":"code","source":["def nn_input(concatenated_original,all_predictions_original):\n","  # Get the last five values in each row of x_test1\n","  last_five_values_x_test1 = concatenated_original[:,-5:,0]\n","  # Create a DataFrame with the last five values and predicted values\n","  data = {'X_test1_Last_1': last_five_values_x_test1[:, 0],\n","          'X_test1_Last_2': last_five_values_x_test1[:, 1],\n","          'X_test1_Last_3': last_five_values_x_test1[:, 2],\n","          'X_test1_Last_4': last_five_values_x_test1[:, 3],\n","          'X_test1_Last_5': last_five_values_x_test1[:, 4],\n","          'Predicted_1': all_predictions_original[:, 0],\n","          'Predicted_2': all_predictions_original[:, 1],\n","          'Predicted_3': all_predictions_original[:, 2],\n","          'Predicted_4': all_predictions_original[:, 3],\n","          'Predicted_5': all_predictions_original[:, 4]}\n","\n","  df = pd.DataFrame(data)\n","\n","  # Specify the path where you want to save the CSV file\n","  csv_path = \"output_file.csv\"\n","\n","  # Save the DataFrame to a CSV file\n","  df.to_csv(csv_path, index=False)"],"metadata":{"id":"yF5SFFaWgZqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Calculate"],"metadata":{"id":"i5G7bx8e3nK0"}},{"cell_type":"code","source":["#parameters\n","sliding_window_size = 10\n","# testing_duration = 2016\n","# training_duration = 2016*3\n","\n","features_df,labels_df =preprocess(df)\n","\n","# Dimentional Reduction\n","num_components = 31\n","pca = PCA(num_components)\n","features_df_pca = pca.fit_transform(features_df) # fit and reduce dimension\n","\n","dataset_size= features_df.shape[0]\n","print(\"Full dataset size: \",dataset_size)\n","testing_duration = int(dataset_size*0.3)\n","training_duration = int(dataset_size*0.7)\n","\n","scaled_x,scaled_y = normalize(features_df,labels_df) #without PCA\n","\n","x_slide, y_slide, y_gan = sliding_window(scaled_x, scaled_y, sliding_window_size)\n","print(f'train_x: {x_slide.shape} train_y: {y_slide.shape} train_y_gan: {y_gan.shape}')\n","\n","train_x_slide,train_y_slide,train_y_gan,test_x_slide,test_y_slide,test_y_gan = split_dataset(x_slide, y_slide, y_gan)\n","print(f'train_x: {train_x_slide.shape} train_y: {train_y_slide.shape} train_y_gan: {train_y_gan.shape}')\n","print(f'test_x: {test_x_slide.shape} test_y: {test_y_slide.shape} test_y_gan: {test_y_gan.shape}')\n","\n","use_cuda = 1\n","cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n","print(device)\n","\n","modelG = train(train_x_slide,train_y_gan,scaled_x,sliding_window_size,device,cuda)\n","evaluateModel(modelG,train_x_slide,test_x_slide,train_y_slide,test_y_slide)\n","\n","\n","#Get predictions to the full dataset\n","concatenated_array = np.concatenate((train_x_slide,test_x_slide), axis=0)\n","# Convert NumPy array to PyTorch tensor\n","concatenated_tensor = torch.tensor(concatenated_array, dtype=torch.float32).to(device)\n","# Forward pass through the model\n","all_predictions, gru_layer = modelG(concatenated_tensor)\n","\n","\n","train_input_features = gru_layer.reshape(gru_layer.shape[0], -1)\n","xgb_input(train_input_features)\n","\n","labels_df = labels_df.to_numpy(dtype=float)\n","features_df = features_df.to_numpy(dtype=float)\n","\n","original,_,_= sliding_window(features_df,labels_df,10)\n","original = original[:training_duration+testing_duration]\n","\n","all_predictions_cpu = all_predictions.cpu().detach()\n","# Convert to NumPy array\n","all_predictions_numpy = all_predictions_cpu.numpy()\n","nn_input(original,all_predictions_numpy)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYC75afK3mV2","executionInfo":{"status":"ok","timestamp":1713373051133,"user_tz":-330,"elapsed":49,"user":{"displayName":"Madara Gunarathna","userId":"05432227937498129950"}},"outputId":"6ee9544d-8243-4fce-d233-871b734ba2a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_x: (12185, 10, 25) train_y: (12185, 5) train_y_gan: (12185, 15, 1)\n","trainX: (6048, 10, 25) trainY: (6048, 5)\n","testX: (2016, 10, 25) testY: (2016, 5)\n","train_x: torch.Size([6048, 10, 25]) train_y: torch.Size([6048, 5]) train_y_gan: torch.Size([6048, 15, 1])\n","test_x: torch.Size([2016, 10, 25]) test_y: torch.Size([2016, 5]) test_y_gan: torch.Size([2016, 15, 1])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wWPAD9iuAICm"},"execution_count":null,"outputs":[]}]}